\begin{frame}
\frametitle{Software and Computing for the HL-LHC Era - Scale}

We don't have precise accounting totals for the global LHC computing effort.
The U.S.\ DOE and NSF jointly invest 
$ \approx \$ 35 $M/year in
ATLAS and CMS software and computing, about half in hardware plus operations,
about half in software professionals.
Extrapolating we can conclude that the LHC funding agencies, worldwide, 
likely invest of order $ \$ 150 $M/year in these enterprises.

In 2014, the LHC experiments used almost 175 PB of tape storage and
slightly more disk storage.
The event rate anticipated for the HL-LHC era is 100 times greater,
and even assuming the experiments significantly reduce the
amount of data stored per event,
the total size of the datasets will be well into the exabyte
scale;
they will be constrained primarily by costs and funding levels,
not by scientific interest.
One long-term goal of a software upgrade for HL-LHC
will be
maximizing the return-on-investment to enable break-through
scientific discoveries using the  HL-LHC detectors.


\end{frame}


